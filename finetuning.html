<!DOCTYPE html>
<html lang="es">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pr√°ctica: Fine Tuning con Axolotl - Dev Blog</title>
    <style>
        :root {
            --bg-body: #0d1117;
            --text-main: #c9d1d9;
            --text-heading: #ffffff;
            --card-bg: #161b22;
            --border: #30363d;
            --accent: #d2a8ff;
            /* Purple for AI/ML */
            --font-main: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            --code-bg: #22272e;
        }

        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            background-color: var(--bg-body);
            color: var(--text-main);
            font-family: var(--font-main);
            line-height: 1.6;
            padding: 2rem;
            max-width: 900px;
            margin: 0 auto;
        }

        header {
            margin-bottom: 3rem;
            padding-bottom: 2rem;
            border-bottom: 1px solid var(--border);
        }

        h1 {
            color: var(--text-heading);
            font-size: 2.5rem;
            margin-bottom: 0.5rem;
        }

        .date {
            font-size: 0.9rem;
            color: #8b949e;
            margin-bottom: 1rem;
            display: block;
        }

        section {
            margin-bottom: 3rem;
        }

        h2 {
            color: var(--accent);
            font-size: 1.8rem;
            margin-bottom: 1.5rem;
            border-bottom: 1px solid rgba(210, 168, 255, 0.2);
            padding-bottom: 0.5rem;
        }

        h3 {
            color: var(--text-heading);
            font-size: 1.3rem;
            margin: 1.5rem 0 1rem;
        }

        p {
            margin-bottom: 1rem;
        }

        .image-container {
            margin: 2rem 0;
            background-color: var(--card-bg);
            border: 1px solid var(--border);
            border-radius: 6px;
            padding: 1rem;
            text-align: center;
        }

        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);
        }

        .caption {
            transform: translateY(10px);
            font-size: 0.85rem;
            color: #8b949e;
            margin-top: 0.5rem;
            font-style: italic;
        }

        code {
            font-family: ui-monospace, SFMono-Regular, "SF Mono", Menlo, Consolas, "Liberation Mono", monospace;
            background-color: rgba(110, 118, 129, 0.4);
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-size: 85%;
        }

        pre {
            background-color: var(--code-bg);
            padding: 1rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1rem;
            border: 1px solid var(--border);
        }

        .alert {
            background-color: rgba(210, 168, 255, 0.1);
            border: 1px solid var(--accent);
            border-radius: 6px;
            padding: 1rem;
            margin: 1.5rem 0;
            color: #e2c0ff;
        }

        .back-link {
            display: inline-block;
            margin-top: 2rem;
            color: var(--accent);
            text-decoration: none;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        .steps-list {
            margin-left: 1.5rem;
            margin-bottom: 1.5rem;
        }

        .steps-list li {
            margin-bottom: 0.5rem;
        }
    </style>
</head>

<body>

    <header>
        <a href="index.html" class="back-link">‚Üê Volver al inicio</a>
        <br><br>
        <h1>Fine Tuning con Axolotl</h1>
        <span class="date">Publicado: 03 de Febrero, 2026</span>
        <p>Documentaci√≥n de errores y configuraci√≥n del entorno para Fine Tuning de LLMs utilizando WSL2 y Axolotl.</p>
    </header>

    <main>

        <section>
            <h2>1. Configuraci√≥n Inicial y Entorno</h2>
            <p>El objetivo de esta pr√°ctica es configurar un entorno adecuado para realizar Fine Tuning. Dado que
                <strong>Axolotl</strong> no es compatible nativamente con Windows, utilizaremos WSL2 (Windows Subsystem
                for Linux) con Ubuntu.
            </p>

            <h3>Activaci√≥n del Entorno Virtual</h3>
            <p>Para activar el entorno de Python en Powershell, ejecutamos:</p>
            <pre><code>venv\Scripts\Activate.ps1</code></pre>
            <p>Es posible encontrarse con errores de pol√≠ticas de ejecuci√≥n. Para solucionarlo, debemos ejecutar:</p>
            <pre><code>Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/error_policy_activacion_venv.png" alt="Error de politicas">
                <p class="caption">Fig 1. Soluci√≥n de errores de pol√≠ticas de ejecuci√≥n al activar el venv.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/creacion_carpeta_finetuning.png" alt="Creaci√≥n de carpeta">
                <p class="caption">Fig 2. Estructura inicial del proyecto.</p>
            </div>
        </section>

        <section>
            <h2>2. Instalaci√≥n de WSL2 y Ubuntu</h2>
            <p>Requerimos Git instalado y una GPU NVIDIA con drivers CUDA. El primer paso cr√≠tico es instalar WSL.</p>

            <ul class="steps-list">
                <li>Instalar WSL.</li>
                <li>Reiniciar el equipo.</li>
                <li>Habilitar virtualizaci√≥n en BIOS si aparece errores de compatibilidad (Intel VT-x / AMD-V).</li>
            </ul>

            <div class="image-container">
                <img src="asi/Finetunning/instalacion_wsl.png" alt="Instalaci√≥n WSL">
                <p class="caption">Fig 3. Proceso de instalaci√≥n de WSL.</p>
            </div>

            <div class="alert">
                <strong>Error Com√∫n:</strong> "WSL2 no es compatible con la configuraci√≥n actual".<br>
                Soluci√≥n: Habilitar "Plataforma de m√°quina virtual" y virtualizaci√≥n en BIOS.
            </div>
        </section>

        <section>
            <h2>3. Verificaci√≥n de GPU y Librer√≠as</h2>
            <p>Una vez dentro de Ubuntu en WSL, verificamos que la GPU sea accesible correctamente mediante
                <code>nvidia-smi</code>.
            </p>

            <div class="image-container">
                <img src="asi/Finetunning/comprobacion_nvidia_smi.png" alt="Nvidia SMI">
                <p class="caption">Fig 4. Comprobaci√≥n correcta de la GPU NVIDIA.</p>
            </div>

            <h3>Instalaci√≥n de PyTorch y Dependencias</h3>
            <p>Instalamos PyTorch con soporte para CUDA 12.1 y las dependencias necesarias para Axolotl.</p>

            <div class="image-container">
                <img src="asi/Finetunning/instalacionPytorch.png" alt="Instalaci√≥n PyTorch">
                <p class="caption">Fig 5. Instalaci√≥n de PyTorch.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/instalacion_pytorch_cuda121.png" alt="CUDA 12.1">
                <p class="caption">Fig 6. Verificaci√≥n de versi√≥n CUDA.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/instalacion_paquetes_conda_axolotl.png" alt="Paquetes Axolotl">
                <p class="caption">Fig 7. Instalaci√≥n de paquetes para Axolotl.</p>
            </div>
        </section>

        <section>
            <h2>4. Informe T√©cnico: Incompatibilidad Hardware "Bleeding Edge"</h2>

            <div class="alert">
                <strong>Dispositivo:</strong> NVIDIA GeForce RTX 5060 Ti<br>
                <strong>Arquitectura:</strong> Blackwell (sm_120)<br>
                <strong>Entorno:</strong> WSL2 (Ubuntu 24.04) sobre Windows 11<br>
                <strong>Estado Final:</strong> Inviable por falta de soporte de Kernel CUDA.
            </div>

            <h3>üìÖ Fase 1: El conflicto del Entorno Base (Python 3.12)</h3>
            <p>El sistema operativo (Ubuntu 24.04) tra√≠a por defecto Python 3.12, lo que desencaden√≥ problemas de
                compatibilidad binaria.</p>

            <h4>1.1. Error de CMake y Versi√≥n de CUDA</h4>
            <div class="alert">
                <code>CMake Error: You've specified CUDA version 124 however the CUDA compiler found is 120.</code>
            </div>
            <p><strong>Causa:</strong> bitsandbytes intentaba compilarse desde el c√≥digo fuente porque no encontraba
                binarios pre-compilados para Python 3.12.</p>
            <p><strong>Soluci√≥n (Variable Injection):</strong> Forzamos las variables de entorno para enga√±ar al
                compilador cmake:</p>
            <pre><code>export BNB_CUDA_VERSION=120
cmake -DCOMPUTE_BACKEND=cuda -DCUDA_VERSION=120 .</code></pre>

            <h4>1.2. Fallo de Compilaci√≥n C++</h4>
            <p>El c√≥digo fuente de bitsandbytes (v0.42) ten√≠a definiciones de tipos de datos incompatibles con los
                headers modernos de CUDA 12.8.</p>

            <h4>1.3. El "Frankenstein" de Librer√≠as</h4>
            <p>Se intent√≥ inyectar un archivo binario <code>.so</code> v√°lido de una versi√≥n antigua renombr√°ndolo, pero
                el sistema detectaba la discrepancia de s√≠mbolos.</p>

            <h3>üìÖ Fase 2: Reingenier√≠a del Entorno (Downgrade a Python 3.10)</h3>
            <p>Debido a la inestabilidad de Python 3.12, se procedi√≥ a montar un entorno paralelo con Python 3.10.</p>

            <h4>Soluci√≥n a ausencia de distutils y Dependency Hell</h4>
            <pre><code>sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt install python3.10-distutils python3.10-dev
pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu124
pip install "unsloth[...] @ git+..." --no-deps</code></pre>

            <h3>üìÖ Fase 3: La Batalla del Enlazado Din√°mico (WSL vs Python)</h3>
            <div class="alert">
                <code>ImportError: .../libcusparse.so.12: undefined symbol: __nvJitLinkComplete_12_4</code>
            </div>
            <p><strong>Causa Cr√≠tica:</strong> Linux cargaba las librer√≠as de sistema de WSL antes que las librer√≠as
                modernas de Python.</p>
            <p><strong>Soluci√≥n:</strong> Reordenamiento de <code>LD_LIBRARY_PATH</code> para priorizar el entorno
                virtual.</p>

            <h3>üìÖ Fase 4: Incompatibilidad de Arquitectura (RTX 5060 Ti)</h3>
            <p>Con el software comunic√°ndose, el problema pas√≥ al hardware f√≠sico (Arquitectura Blackwell sm_120).</p>

            <h4>4.1. Error de Atributo int1</h4>
            <p>Se elimin√≥ la librer√≠a experimental <code>torchao</code> que causaba conflictos con PyTorch 2.5.1.</p>

            <h4>4.2. Arquitectura No Soportada (sm_120)</h4>
            <div class="alert">
                <code>NVIDIA GeForce RTX 5060 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.</code>
            </div>
            <p><strong>Soluci√≥n (Spoofing):</strong> Enga√±amos a PyTorch para que tratara a la 5060 Ti como una H100
                (sm_90):</p>
            <pre><code>export TORCH_CUDA_ARCH_LIST="9.0"</code></pre>

            <h4>4.3. Falta de Kernel Binario (El Fin del Camino)</h4>
            <div class="alert"
                style="border-color: #ff7b72; background-color: rgba(255, 123, 114, 0.1); color: #ff7b72;">
                <strong>FATAL ERROR:</strong> RuntimeError: CUDA error: no kernel image is available for execution on
                the device
            </div>
            <p>A pesar de enga√±ar al software, al intentar ejecutar operaciones matem√°ticas reales, el driver busc√≥ el
                c√≥digo binario para <strong>sm_120</strong>, el cual no existe en PyTorch Stable (CUDA 12.4) ni es
                estable en Nightly (CUDA 12.8).</p>

            <h3>üõ†Ô∏è Resumen de Manipulaciones</h3>
            <ul class="steps-list">
                <li><strong>Edici√≥n de C√≥digo:</strong> Parcheo de <code>__init__.py</code> de Unsloth.</li>
                <li><strong>Inyecci√≥n de Binarios:</strong> Spoofing de archivos <code>.so</code>.</li>
                <li><strong>Variables de Entorno:</strong> <code>BNB_CUDA_VERSION</code>, <code>LD_LIBRARY_PATH</code>,
                    <code>TORCH_CUDA_ARCH_LIST</code>.
                </li>
                <li><strong>Gesti√≥n de Paquetes:</strong> Uso de flags <code>--no-deps</code> y repositorios mixtos.
                </li>
            </ul>

            <div class="image-container">
                <img src="asi/Finetunning/aviso_compatibilidad_cuda_sm120_mistral.png" alt="Aviso Compatibilidad">
                <p class="caption">Fig 8. Aviso de incompatibilidad detectado inicialmente.</p>
            </div>

            <p><strong>Conclusi√≥n:</strong> La RTX 5060 Ti es funcional a nivel hardware, pero el ecosistema de software
                (PyTorch + Bitsandbytes + CUDA en WSL) tiene un retraso de implementaci√≥n respecto al lanzamiento del
                hardware "Bleeding Edge".</p>
        </section>

        <!-- ===== NUEVA SECCI√ìN: FINE-TUNING EN APPLE SILICON ===== -->

        <section>
            <h2>5. Alternativa: Fine-Tuning en Apple Silicon (Mac con MLX)</h2>
            <p>Ante la incompatibilidad de la RTX 5060 Ti, se adapt√≥ el flujo de trabajo para ejecutarlo nativamente en
                un
                <strong>Mac (Apple Silicon)</strong> utilizando la librer√≠a <strong>MLX</strong> de Apple, que aprovecha
                la
                GPU y la memoria unificada del chip M1/M2/M3.
            </p>

            <div class="alert">
                <strong>Objetivo:</strong> Realizar un Fine-Tuning sobre Mistral-7B para convertirlo en un asistente
                experto en formalidad, capaz de responder siempre utilizando el tratamiento de "usted" y un tono
                profesional.
            </div>
        </section>

        <section>
            <h2>6. Preparaci√≥n del Entorno en Mac</h2>
            <p>Se configur√≥ el entorno de trabajo con las herramientas espec√≠ficas para Apple Silicon:</p>

            <ul class="steps-list">
                <li>Crear directorio de trabajo: <code>cd /Users/.../FineTuningASI</code></li>
                <li>Crear y activar entorno virtual: <code>python3 -m venv venv && source venv/bin/activate</code></li>
                <li>Configurar cach√© de modelos: <code>export HF_HOME=$(pwd)/cache</code></li>
                <li>Instalar librer√≠as: <code>pip install mlx-lm huggingface_hub</code></li>
            </ul>

            <div class="image-container">
                <img src="asi/Finetunning/mac_instalacion_entorno_mlx.png" alt="Instalaci√≥n entorno MLX">
                <p class="caption">Fig 9. Creaci√≥n del entorno virtual e instalaci√≥n de mlx-lm y huggingface_hub en Mac.
                </p>
            </div>
        </section>

        <section>
            <h2>7. Creaci√≥n y Estructuraci√≥n del Dataset</h2>
            <p>Se cre√≥ un dataset de 20 ejemplos en formato <strong>JSONL</strong> con el formato de instrucci√≥n de
                Mistral
                (<code>&lt;s&gt;[INST] ... [/INST] ... &lt;/s&gt;</code>).</p>

            <h3>Formato del Dataset</h3>
            <p>Cada ejemplo contiene un campo <code>text</code> con la instrucci√≥n y la respuesta formal esperada:</p>
            <pre><code>{"text": "&lt;s&gt;[INST] Hola, ¬øqui√©n eres? [/INST] Buenas tardes. Soy un asistente virtual dise√±ado para ayudarle. ¬øEn qu√© puedo servirle hoy? &lt;/s&gt;"}</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_edicion_dataset_jsonl.png" alt="Edici√≥n del dataset JSONL">
                <p class="caption">Fig 10. Edici√≥n del archivo <code>datos_asistente_formal.jsonl</code> con ejemplos de
                    entrenamiento.</p>
            </div>

            <p>Se generaron los archivos <code>train.jsonl</code>, <code>valid.jsonl</code> y <code>test.jsonl</code>
                (copias
                del original) para satisfacer los requisitos de la herramienta de entrenamiento.</p>
        </section>

        <section>
            <h2>8. Entrenamiento del Modelo (Fine-Tuning con LoRA)</h2>
            <p>Se utiliz√≥ <strong>LoRA (Low-Rank Adaptation)</strong> para ajustar solo las capas de atenci√≥n del modelo
                base <code>mistralai/Mistral-7B-v0.1</code>, reduciendo as√≠ el consumo de RAM.</p>

            <h3>Error 1: Training set not found</h3>
            <div class="alert">
                <code>ValueError: Training set not found or empty.</code>
            </div>
            <p><strong>Causa:</strong> El archivo de datos se llamaba <code>datos_asistente_formal.jsonl</code> en lugar
                del nombre esperado por defecto.</p>
            <p><strong>Soluci√≥n:</strong> Renombrar a <code>train.jsonl</code>:</p>
            <pre><code>mv datos_asistente_formal.jsonl train.jsonl</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_error_training_set_not_found.png" alt="Error training set">
                <p class="caption">Fig 11. Error: el script no encuentra el dataset de entrenamiento.</p>
            </div>

            <h3>Error 2: Validation set not found</h3>
            <div class="alert">
                <code>ValueError: Validation set not found or empty.</code>
            </div>
            <p><strong>Soluci√≥n:</strong> Crear copias del archivo para validaci√≥n y test:</p>
            <pre><code>cp train.jsonl valid.jsonl
cp train.jsonl test.jsonl</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_error_validation_set_not_found.png" alt="Error validation set">
                <p class="caption">Fig 12. Segundo error: falta el conjunto de validaci√≥n.</p>
            </div>

            <h3>Entrenamiento Exitoso</h3>
            <p>Con los archivos correctamente nombrados, se ejecut√≥ el entrenamiento LoRA con √©xito:</p>
            <pre><code>python -m mlx_lm.lora \
  --model mistralai/Mistral-7B-v0.1 \
  --train \
  --data ./ \
  --iters 600 \
  --batch-size 1 \
  --num-layers 4 \
  --adapter-file adapters.npz</code></pre>
            <p>El entrenamiento complet√≥ las iteraciones bajando el <em>loss</em> de 1.4 a 0.2, con un uso de memoria de
                ~14.88 GB.</p>

            <div class="image-container">
                <img src="asi/Finetunning/mac_entrenamiento_lora_exitoso.png" alt="Entrenamiento LoRA exitoso">
                <p class="caption">Fig 13. Entrenamiento LoRA completado con √©xito. Los pesos se guardan en
                    <code>adapters.safetensors</code>.</p>
            </div>
        </section>

        <section>
            <h2>9. Fusi√≥n y Conversi√≥n del Modelo a GGUF</h2>

            <h3>9.1. Clonaci√≥n de llama.cpp</h3>
            <p>Para convertir el modelo a formato GGUF (compatible con LM Studio), clonamos el repositorio
                <code>llama.cpp</code>:</p>

            <div class="image-container">
                <img src="asi/Finetunning/mac_clonacion_llama_cpp.png" alt="Clonaci√≥n llama.cpp">
                <p class="caption">Fig 14. Clonaci√≥n del repositorio llama.cpp.</p>
            </div>

            <h3>Error: Dependencias incompatibles</h3>
            <div class="alert">
                <code>ERROR: Could not find a version that satisfies the requirement matplotlib&lt;3.10.0</code>
            </div>
            <p><strong>Soluci√≥n:</strong> Instalar solo las dependencias necesarias manualmente:</p>
            <pre><code>pip install numpy sentencepiece transformers gguf protobuf</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_error_dependencias_llama_cpp.png" alt="Error dependencias">
                <p class="caption">Fig 15. Conflicto de versiones al instalar requisitos de llama.cpp.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/mac_instalacion_dependencias_manual.png" alt="Instalaci√≥n manual">
                <p class="caption">Fig 16. Instalaci√≥n manual de dependencias necesarias.</p>
            </div>

            <h3>9.2. Fusi√≥n del Modelo (Merge)</h3>
            <p>Combinaci√≥n de los pesos base con los adaptadores LoRA entrenados:</p>
            <pre><code>python -m mlx_lm.fuse \
  --model mistralai/Mistral-7B-v0.1 \
  --adapter-file adapters.npz \
  --save-path fused_model</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_fusion_modelo_lora.png" alt="Fusi√≥n del modelo">
                <p class="caption">Fig 17. Fusi√≥n del adaptador LoRA con el modelo base Mistral-7B.</p>
            </div>

            <h3>9.3. Conversi√≥n a GGUF</h3>

            <h4>Error: torch no encontrado</h4>
            <div class="alert">
                <code>ModuleNotFoundError: No module named 'torch'</code>
            </div>
            <p><strong>Soluci√≥n:</strong> <code>pip install torch</code></p>

            <div class="image-container">
                <img src="asi/Finetunning/mac_error_torch_no_encontrado.png" alt="Error torch">
                <p class="caption">Fig 18. Error al ejecutar la conversi√≥n: falta torch.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/mac_instalacion_torch.png" alt="Instalaci√≥n torch">
                <p class="caption">Fig 19. Instalaci√≥n de torch y dependencias asociadas.</p>
            </div>

            <h4>Error: Tipo de cuantizaci√≥n inv√°lido</h4>
            <div class="alert">
                <code>argument --outtype: invalid choice: 'q4_k_m' (choose from 'f32', 'f16', 'bf16', 'q8_0', 'auto')</code>
            </div>
            <p><strong>Soluci√≥n:</strong> Usar <code>f16</code> en lugar de <code>q4_k_m</code> para mantener calidad:
            </p>
            <pre><code>python llama.cpp/convert_hf_to_gguf.py modelo_fusionado \
  --outfile modelo-formal.gguf --outtype f16</code></pre>

            <div class="image-container">
                <img src="asi/Finetunning/mac_error_tipo_cuantizacion.png" alt="Error cuantizaci√≥n">
                <p class="caption">Fig 20. El formato q4_k_m no est√° disponible; se usa f16.</p>
            </div>

            <h3>Conversi√≥n Exitosa</h3>

            <div class="image-container">
                <img src="asi/Finetunning/mac_conversion_gguf_exitosa.png" alt="Conversi√≥n GGUF exitosa">
                <p class="caption">Fig 21. Conversi√≥n del modelo fusionado a formato GGUF en progreso.</p>
            </div>

            <div class="image-container">
                <img src="asi/Finetunning/mac_modelo_exportado_final.png" alt="Modelo exportado">
                <p class="caption">Fig 22. Modelo exportado exitosamente: <code>modelo-formal.gguf</code> (~14.5 GB).
                </p>
            </div>
        </section>

        <section>
            <h2>10. Resultados y Conclusiones del Fine-Tuning en Mac</h2>

            <h3>Pruebas Comparativas</h3>
            <p>Se realizaron pruebas en <strong>LM Studio</strong> comparando el modelo base con el modelo ajustado:</p>

            <div class="alert"
                style="border-color: #7ee787; background-color: rgba(126, 231, 135, 0.1); color: #7ee787;">
                <strong>Modelo Base (sin fine-tuning):</strong><br>
                Pregunta: "¬øMe puedes decir la hora?"<br>
                Respuesta: "Claro, son las 10:30" <em>(Tono informal)</em>
            </div>

            <div class="alert"
                style="border-color: #d2a8ff; background-color: rgba(210, 168, 255, 0.1); color: #e2c0ff;">
                <strong>Modelo Ajustado (fine-tuned):</strong><br>
                Pregunta: "¬øMe puedes decir la hora?"<br>
                Respuesta: "Buenas tardes. Actualmente son exactamente las diez y media. ¬øDesea usted saber algo m√°s?"
                <em>(Tono formal, tratamiento de "usted")</em>
            </div>

            <h3>Conclusiones</h3>
            <ul class="steps-list">
                <li><strong>Viabilidad en Apple Silicon:</strong> El hardware M1/M2/M3 es capaz de entrenar modelos de
                    7B
                    par√°metros eficientemente gracias a la memoria unificada.</li>
                <li><strong>Importancia del Dataset:</strong> La estructuraci√≥n correcta de archivos <code>.jsonl</code>
                    y
                    sus nombres est√°ndar (<code>train</code>, <code>valid</code>, <code>test</code>) es cr√≠tica.</li>
                <li><strong>Eficiencia de LoRA:</strong> Ajustar solo las capas de atenci√≥n (4 capas) es suficiente para
                    cambiar completamente el estilo de respuesta del modelo.</li>
                <li><strong>Gesti√≥n de Dependencias:</strong> Es vital resolver conflictos de versiones entre librer√≠as
                    din√°micas como <code>mlx</code>, <code>torch</code> y <code>llama.cpp</code>.</li>
                <li><strong>Formato GGUF:</strong> La conversi√≥n a f16 produce un modelo de ~14.5 GB funcional en LM
                    Studio, aunque sin cuantizaci√≥n adicional.</li>
            </ul>
        </section>

    </main>

    <footer>
        <p>¬© 2026 - Documentaci√≥n Generada para ASI</p>
    </footer>

</body>

</html>